{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08c98da6-dd6a-45c3-9864-c682188a6294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8772282b-315b-472d-93bb-6a0d9d60064b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a3a4d0-b39c-41f1-9db9-5940b84f2060",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"standardcharges.csv\"\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    for i in range(2000):\n",
    "        print(f.readline().strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b707ded4-3d4f-4411-80ad-5eccb7532b58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2246a3e3-1a2e-4dc3-9d3c-6eacce49ed30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hospital_name,last_updated_on,version,hospital_location,hospital_address,financial_aid_policy,license_number|NC,\"To the best of its knowledge and belief, the hospital has included all applicable standard charge information in accordance with the requirements of 45 CFR 180.50, and the information encoded is true, accurate, and complete as of the date indicated.\",general_contract_provisions\n",
      "The Charlotte Mecklenburg Hospital Authority,2024-12-01,2.0.0,Atrium Health Anson,\"2301 US Hwy 74 West, Wadesboro, NC 28170\",,H0082,TRUE,\n",
      "description,code|1,code|1|type,code|2,code|2|type,code|3,code|3|type,code|4,code|4|type,billing_class,setting,drug_unit_of_measurement,drug_type_of_measurement,modifiers,standard_charge|gross,standard_charge|discounted_cash,payer_name,plan_name,standard_charge|negotiated_dollar,standard_charge|negotiated_percentage,standard_charge|negotiated_algorithm,estimated_amount,standard_charge|methodology,standard_charge|min,standard_charge|max,additional_generic_notes,additional_payer_notes\n",
      "LENS IOL ACRYSOF IQ SA6AT5 1-PC 14.5 DIOPTER 3 CYLIND 6MM 13MM POST CHAMB TORIC ANT BICONVEX,258094,CDM,V2787,HCPCS,0276,RC,,,facility,inpatient,,,,495.00,247.50,Aetna,Broad Network,269.78,54.50,,,percent of total billed charges,151.47,470.25,,\n",
      "REAGENT ARCHITCT TRIGG SOLN 25TST 4X1 L C4100/CI16200/CI8200/I1000SR/I2000/I2000SR,116575,CDM,,,,,,,facility,inpatient,,,,16.29,8.15,Aetna,Broad Network,8.88,54.50,,,percent of total billed charges,4.98,15.48,,\n"
     ]
    }
   ],
   "source": [
    "with open(\".csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in range(5):\n",
    "        print(f.readline().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b7538a",
   "metadata": {},
   "source": [
    "Loading data into smaller chunked files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31e1fc0-b66d-48de-b934-edcae77cf007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: charlotte_mecklenburg_hospital_cleaned_chunks//charlotte_mecklenburg_hospital_cleaned_part_0.csv (101895 rows)\n",
      "✅ Saved: charlotte_mecklenburg_hospital_cleaned_chunks//charlotte_mecklenburg_hospital_cleaned_part_1.csv (103316 rows)\n",
      "✅ Saved: charlotte_mecklenburg_hospital_cleaned_chunks//charlotte_mecklenburg_hospital_cleaned_part_2.csv (101199 rows)\n",
      "✅ Saved: charlotte_mecklenburg_hospital_cleaned_chunks//charlotte_mecklenburg_hospital_cleaned_part_3.csv (102613 rows)\n",
      "✅ Saved: charlotte_mecklenburg_hospital_cleaned_chunks//charlotte_mecklenburg_hospital_cleaned_part_4.csv (7995 rows)\n",
      "✅ All processing complete. Chunked files saved consistently in: charlotte_mecklenburg_hospital_cleaned_chunks/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# === CONFIG ===\n",
    "input_file = \"standardcharges.csv\"\n",
    "output_dir = \"cleaned_chunks/\"\n",
    "header_row = 2\n",
    "rows_per_file = 100000  \n",
    "\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "columns_to_drop = [\n",
    "    \"code|3\", \"code|3|type\", \"code|4\", \"code|4|type\",\n",
    "    \"billing_class\", \"setting\", \"drug_unit_of_measurement\", \"drug_type_of_measurement\",\n",
    "    \"modifiers\", \"standard_charge|gross\", \"standard_charge|discounted_cash\",\n",
    "    \"standard_charge|negotiated_algorithm\", \"estimated_amount\", \"standard_charge|methodology\",\n",
    "    \"additional_generic_notes\", \"additional_payer_notes\"\n",
    "]\n",
    "\n",
    "# Final column name mapping\n",
    "columns_mapping = {\n",
    "    'description': 'DESCRIPTION',\n",
    "    'code|1': 'CODE',\n",
    "    'code|1|type': 'CODE_TYPE',\n",
    "    'payer_name': 'PAYER_NAME',\n",
    "    'plan_name': 'PLAN_NAME',\n",
    "    'standard_charge|negotiated_dollar': 'STANDARD_CHARGE_DOLLAR',\n",
    "    'standard_charge|min': 'MINIMUM_CHARGE',\n",
    "    'standard_charge|max': 'MAXIMUM_CHARGE'\n",
    "}\n",
    "\n",
    "# CHUNK PROCESSING\n",
    "def process_chunk(chunk):\n",
    "    \n",
    "    mask = (chunk[\"code|1|type\"] == \"CDM\") & (chunk[\"code|2|type\"] == \"CPT\")\n",
    "    chunk.loc[mask, \"code|1\"] = chunk.loc[mask, \"code|2\"]\n",
    "    chunk.loc[mask, \"code|1|type\"] = \"CPT\"\n",
    "\n",
    "    \n",
    "    chunk = chunk[chunk[\"code|1|type\"] == \"CPT\"]\n",
    "\n",
    "   \n",
    "    chunk = chunk.drop(columns=[\"code|2\", \"code|2|type\"], errors=\"ignore\")\n",
    "    chunk = chunk.drop(columns=[col for col in columns_to_drop if col in chunk.columns])\n",
    "\n",
    "    \n",
    "    dollar_col = \"standard_charge|negotiated_dollar\"\n",
    "    percent_col = \"standard_charge|negotiated_percentage\"\n",
    "    max_col = \"standard_charge|max\"\n",
    "\n",
    "    chunk[dollar_col] = pd.to_numeric(chunk[dollar_col], errors=\"coerce\")\n",
    "    chunk[percent_col] = pd.to_numeric(chunk[percent_col], errors=\"coerce\")\n",
    "    chunk[max_col] = pd.to_numeric(chunk[max_col], errors=\"coerce\")\n",
    "\n",
    "   \n",
    "    missing_both = chunk[dollar_col].isna() & chunk[percent_col].isna()\n",
    "    chunk = chunk[~missing_both]\n",
    "\n",
    "    \n",
    "    missing_dollar = chunk[dollar_col].isna() & chunk[percent_col].notna() & chunk[max_col].notna()\n",
    "    chunk.loc[missing_dollar, dollar_col] = (\n",
    "        chunk.loc[missing_dollar, percent_col] / 100 * chunk.loc[missing_dollar, max_col]\n",
    "    )\n",
    "\n",
    "   \n",
    "    chunk = chunk.drop(columns=[percent_col], errors=\"ignore\")\n",
    "\n",
    "   \n",
    "    chunk.rename(columns={k: v for k, v in columns_mapping.items() if k in chunk.columns}, inplace=True)\n",
    "\n",
    "    return chunk\n",
    "\n",
    "\n",
    "reader = pd.read_csv(input_file, header=header_row, chunksize=100000, low_memory=False)\n",
    "buffer = []\n",
    "total_rows = 0\n",
    "file_index = 0\n",
    "\n",
    "for chunk in reader:\n",
    "    cleaned = process_chunk(chunk)\n",
    "    if cleaned.empty:\n",
    "        continue\n",
    "\n",
    "    buffer.append(cleaned)\n",
    "    total_rows += len(cleaned)\n",
    "\n",
    "    if total_rows >= rows_per_file:\n",
    "        combined = pd.concat(buffer)\n",
    "\n",
    "        out_file = f\"{output_dir}/cleaned_part_{file_index}.csv\"\n",
    "        combined.to_csv(out_file, index=False)\n",
    "        print(f\"Saved: {out_file} ({len(combined)} rows)\")\n",
    "\n",
    "        buffer = []\n",
    "        total_rows = 0\n",
    "        file_index += 1\n",
    "\n",
    "# Save any remaining rows (same format)\n",
    "if buffer:\n",
    "    combined = pd.concat(buffer)\n",
    "\n",
    "    out_file = f\"{output_dir}/cleaned_part_{file_index}.csv\"\n",
    "    combined.to_csv(out_file, index=False)\n",
    "    print(f\"Saved: {out_file} ({len(combined)} rows)\")\n",
    "\n",
    "print(\"All processing complete. Chunked files saved consistently in:\", output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e99b71b-ff72-41c8-8c97-69979f3cd269",
   "metadata": {},
   "source": [
    "Loading data into a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cf8557-8b5f-4edf-bfc9-1ace588a7cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed chunk #1 with 3081 rows.\n",
      "✅ Processed chunk #2 with 3225 rows.\n",
      "✅ Processed chunk #3 with 3147 rows.\n",
      "✅ Processed chunk #4 with 3118 rows.\n",
      "✅ Processed chunk #5 with 3042 rows.\n",
      "✅ Processed chunk #6 with 3050 rows.\n",
      "✅ Processed chunk #7 with 3893 rows.\n",
      "✅ Processed chunk #8 with 4035 rows.\n",
      "✅ Processed chunk #9 with 5349 rows.\n",
      "✅ Processed chunk #10 with 4565 rows.\n",
      "✅ Processed chunk #11 with 12455 rows.\n",
      "✅ Processed chunk #12 with 3066 rows.\n",
      "✅ Processed chunk #13 with 3157 rows.\n",
      "✅ Processed chunk #14 with 3125 rows.\n",
      "✅ Processed chunk #15 with 3097 rows.\n",
      "✅ Processed chunk #16 with 3106 rows.\n",
      "✅ Processed chunk #17 with 3099 rows.\n",
      "✅ Processed chunk #18 with 3076 rows.\n",
      "✅ Processed chunk #19 with 3897 rows.\n",
      "✅ Processed chunk #20 with 3372 rows.\n",
      "✅ Processed chunk #21 with 11801 rows.\n",
      "✅ Processed chunk #22 with 3127 rows.\n",
      "✅ Processed chunk #23 with 3141 rows.\n",
      "✅ Processed chunk #24 with 3150 rows.\n",
      "✅ Processed chunk #25 with 3046 rows.\n",
      "✅ Processed chunk #26 with 3189 rows.\n",
      "✅ Processed chunk #27 with 3069 rows.\n",
      "✅ Processed chunk #28 with 3137 rows.\n",
      "✅ Processed chunk #29 with 3101 rows.\n",
      "✅ Processed chunk #30 with 3797 rows.\n",
      "✅ Processed chunk #31 with 11826 rows.\n",
      "✅ Processed chunk #32 with 3165 rows.\n",
      "✅ Processed chunk #33 with 3097 rows.\n",
      "✅ Processed chunk #34 with 3048 rows.\n",
      "✅ Processed chunk #35 with 3291 rows.\n",
      "✅ Processed chunk #36 with 3076 rows.\n",
      "✅ Processed chunk #37 with 3080 rows.\n",
      "✅ Processed chunk #38 with 3099 rows.\n",
      "✅ Processed chunk #39 with 3120 rows.\n",
      "✅ Processed chunk #40 with 5770 rows.\n",
      "✅ Processed chunk #41 with 3154 rows.\n",
      "✅ Processed chunk #42 with 4014 rows.\n",
      "✅ Processed chunk #43 with 3127 rows.\n",
      "✅ Processed chunk #44 with 3055 rows.\n",
      "✅ Processed chunk #45 with 3184 rows.\n",
      "✅ Processed chunk #46 with 3121 rows.\n",
      "✅ Processed chunk #47 with 3129 rows.\n",
      "✅ Processed chunk #48 with 11738 rows.\n",
      "✅ Processed chunk #49 with 3105 rows.\n",
      "✅ Processed chunk #50 with 3975 rows.\n",
      "✅ Processed chunk #51 with 3158 rows.\n",
      "✅ Processed chunk #52 with 3099 rows.\n",
      "✅ Processed chunk #53 with 3222 rows.\n",
      "✅ Processed chunk #54 with 3075 rows.\n",
      "✅ Processed chunk #55 with 3561 rows.\n",
      "✅ Processed chunk #56 with 3974 rows.\n",
      "✅ Processed chunk #57 with 3887 rows.\n",
      "✅ Processed chunk #58 with 3796 rows.\n",
      "✅ Processed chunk #59 with 3825 rows.\n",
      "✅ Processed chunk #60 with 4137 rows.\n",
      "✅ Processed chunk #61 with 12011 rows.\n",
      "✅ Processed chunk #62 with 3216 rows.\n",
      "✅ Processed chunk #63 with 3124 rows.\n",
      "✅ Processed chunk #64 with 3054 rows.\n",
      "✅ Processed chunk #65 with 3237 rows.\n",
      "✅ Processed chunk #66 with 3106 rows.\n",
      "✅ Processed chunk #67 with 3095 rows.\n",
      "✅ Processed chunk #68 with 3148 rows.\n",
      "✅ Processed chunk #69 with 3061 rows.\n",
      "✅ Processed chunk #70 with 3212 rows.\n",
      "✅ Processed chunk #71 with 4431 rows.\n",
      "✅ Processed chunk #72 with 13495 rows.\n",
      "✅ Processed chunk #73 with 13475 rows.\n",
      "✅ Processed chunk #74 with 3033 rows.\n",
      "✅ Processed chunk #75 with 3136 rows.\n",
      "✅ Processed chunk #76 with 3217 rows.\n",
      "✅ Processed chunk #77 with 3865 rows.\n",
      "✅ Processed chunk #78 with 3070 rows.\n",
      "✅ Processed chunk #79 with 3105 rows.\n",
      "✅ Processed chunk #80 with 3120 rows.\n",
      "✅ Processed chunk #81 with 3106 rows.\n",
      "✅ Processed chunk #82 with 3883 rows.\n",
      "✅ Processed chunk #83 with 3204 rows.\n",
      "✅ Processed chunk #84 with 11767 rows.\n",
      "✅ Processed chunk #85 with 3089 rows.\n",
      "✅ Processed chunk #86 with 3180 rows.\n",
      "✅ Processed chunk #87 with 3158 rows.\n",
      "✅ Processed chunk #88 with 3013 rows.\n",
      "✅ Processed chunk #89 with 3173 rows.\n",
      "✅ Processed chunk #90 with 3147 rows.\n",
      "✅ Processed chunk #91 with 3073 rows.\n",
      "✅ Processed chunk #92 with 3206 rows.\n",
      "✅ Processed chunk #93 with 3117 rows.\n",
      "✅ Processed chunk #94 with 3111 rows.\n",
      "✅ Processed chunk #95 with 3110 rows.\n",
      "✅ Processed chunk #96 with 3131 rows.\n",
      "✅ Processed chunk #97 with 3173 rows.\n",
      "✅ Processed chunk #98 with 3145 rows.\n",
      "✅ Processed chunk #99 with 3046 rows.\n",
      "✅ Processed chunk #100 with 7972 rows.\n",
      "✅ Processed chunk #101 with 6980 rows.\n",
      "✅ Processed chunk #102 with 3174 rows.\n",
      "✅ Processed chunk #103 with 3452 rows.\n",
      "✅ Processed chunk #104 with 3083 rows.\n",
      "✅ Processed chunk #105 with 3044 rows.\n",
      "✅ Processed chunk #106 with 11858 rows.\n",
      "✅ Processed chunk #107 with 3204 rows.\n",
      "✅ Processed chunk #108 with 4537 rows.\n",
      "✅ Processed chunk #109 with 3658 rows.\n",
      "✅ Processed chunk #110 with 4117 rows.\n",
      "✅ Processed chunk #111 with 3940 rows.\n",
      "✅ Processed chunk #112 with 4109 rows.\n",
      "✅ Processed chunk #113 with 5342 rows.\n",
      "✅ Processed chunk #114 with 12326 rows.\n",
      "✅ Processed chunk #115 with 3119 rows.\n",
      "✅ Processed chunk #116 with 12733 rows.\n",
      "✅ Processed chunk #117 with 7996 rows.\n",
      "\n",
      "🎉 All chunks processed. Final file saved as:\n",
      "charlotte_mecklenburg_hospital_cleaned/charlotte_mecklenburg_hospital_cleaned_all.csv\n",
      "📊 Total rows saved: 506108\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "input_file = \"standardcharges.csv\"\n",
    "output_dir = \"cleaned/\"\n",
    "output_file = os.path.join(output_dir, \"cleaned_all.csv\")\n",
    "header_row = 2\n",
    "chunk_size = 100000\n",
    "\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "columns_to_drop = [\n",
    "    \"code|3\", \"code|3|type\", \"code|4\", \"code|4|type\",\n",
    "    \"billing_class\", \"setting\", \"drug_unit_of_measurement\", \"drug_type_of_measurement\",\n",
    "    \"modifiers\", \"standard_charge|gross\", \"standard_charge|discounted_cash\",\n",
    "    \"standard_charge|negotiated_algorithm\", \"estimated_amount\", \"standard_charge|methodology\",\n",
    "    \"additional_generic_notes\", \"additional_payer_notes\"\n",
    "]\n",
    "\n",
    "\n",
    "columns_mapping = {\n",
    "    'description': 'DESCRIPTION',\n",
    "    'code|1': 'CODE',\n",
    "    'code|1|type': 'CODE_TYPE',\n",
    "    'payer_name': 'PAYER_NAME',\n",
    "    'plan_name': 'PLAN_NAME',\n",
    "    'standard_charge|negotiated_dollar': 'STANDARD_CHARGE_DOLLAR',\n",
    "    'standard_charge|min': 'MINIMUM_CHARGE',\n",
    "    'standard_charge|max': 'MAXIMUM_CHARGE',\n",
    "    'standard_charge|negotiated_percentage': 'STANDARD_CHARGE_PERCENTAGE'\n",
    "}\n",
    "\n",
    "\n",
    "def process_chunk(chunk, chunk_number):\n",
    "    \n",
    "    mask = (chunk[\"code|1|type\"] == \"CDM\") & (chunk[\"code|2|type\"] == \"CPT\")\n",
    "    chunk.loc[mask, \"code|1\"] = chunk.loc[mask, \"code|2\"]\n",
    "    chunk.loc[mask, \"code|1|type\"] = \"CPT\"\n",
    "\n",
    "    \n",
    "    chunk = chunk[chunk[\"code|1|type\"] == \"CPT\"]\n",
    "\n",
    "    \n",
    "    chunk = chunk.drop(columns=[\"code|2\", \"code|2|type\"], errors=\"ignore\")\n",
    "    chunk = chunk.drop(columns=[col for col in columns_to_drop if col in chunk.columns], errors=\"ignore\")\n",
    "\n",
    "    \n",
    "    dollar_col = \"standard_charge|negotiated_dollar\"\n",
    "    percent_col = \"standard_charge|negotiated_percentage\"\n",
    "    max_col = \"standard_charge|max\"\n",
    "\n",
    "    chunk[dollar_col] = pd.to_numeric(chunk[dollar_col], errors=\"coerce\")\n",
    "    chunk[percent_col] = pd.to_numeric(chunk[percent_col], errors=\"coerce\").fillna(0)\n",
    "    chunk[max_col] = pd.to_numeric(chunk[max_col], errors=\"coerce\")\n",
    "\n",
    "    \n",
    "    missing_dollar = chunk[dollar_col].isna() & chunk[max_col].notna()\n",
    "    chunk.loc[missing_dollar, dollar_col] = (\n",
    "        chunk.loc[missing_dollar, percent_col] / 100 * chunk.loc[missing_dollar, max_col]\n",
    "    )\n",
    "\n",
    "    \n",
    "    chunk.rename(columns={k: v for k, v in columns_mapping.items() if k in chunk.columns}, inplace=True)\n",
    "\n",
    "    print(f\"Processed chunk #{chunk_number} with {len(chunk)} rows.\")\n",
    "    return chunk\n",
    "\n",
    "\n",
    "reader = pd.read_csv(input_file, header=header_row, chunksize=chunk_size, low_memory=False)\n",
    "all_cleaned = []\n",
    "chunk_number = 0\n",
    "\n",
    "for chunk in reader:\n",
    "    chunk_number += 1\n",
    "    cleaned = process_chunk(chunk, chunk_number)\n",
    "    if not cleaned.empty:\n",
    "        all_cleaned.append(cleaned)\n",
    "\n",
    "# Combining all chunks and write to a single file\n",
    "if all_cleaned:\n",
    "    final_df = pd.concat(all_cleaned, ignore_index=True)\n",
    "    final_df.to_csv(output_file, index=False)\n",
    "    print(f\"All chunks processed. Final file saved as:\\n{output_file}\")\n",
    "    print(f\"Total rows saved: {len(final_df)}\")\n",
    "else:\n",
    "    print(\"No valid data to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0522c50e-7b6e-438c-82be-3086126a7d57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
